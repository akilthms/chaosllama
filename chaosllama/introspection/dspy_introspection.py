"""
The purpose of this class is to inspect the data intelligence generated by mlflow.evaluate & lakehouse monitor to determine how to optimize the System prompt in One Shot

TODO: Attempt other strategies for metadata update. Here are a few strategies:
* Update prompt incrementally, one by one, per feedback of each quesiton in the evaluation dataset
"""

import dspy
from databricks.connect import DatabricksSession
from dspy.datasets.dataset import Dataset as DspyDataset
from sklearn.model_selection import train_test_split
from typing import Dict, Optional, List, Callable, Literal, Union
import pandas as pd
from abc import ABC
from dataclasses import dataclass, asdict
from dotenv import dotenv_values

import mlflow
from mlflow.genai.scorers import Guidelines
from mlflow.genai.judges import meets_guidelines

from mlflow.entities import Feedback

from chaosllama.services.evaluation_dataset import EvalSetManager
from chaosllama.services.genie import GenieAgent, GenieService
from chaosllama.profiles.config import config
from chaosllama.services.judges import JudgeService
from chaosllama.entities.models import (
                                        AgentConfig,
                                        IntrospectionManager,
                                        AgentInput,
                                        MosaicJudgesInput,
                                        DspyFeedback
                                        )
from chaosllama.prompts import registry as prompt_registry



class IntrospectiveAI(ABC):
    def __init__(self, agent_config: AgentConfig):
        self.agent_config = agent_config

    def introspect(self, data_intelligence: pd.DataFrame):
        pass

    def optimize(self,
                 agent_input,
                 mode="system_instructions") -> Optional[Union[str, pd.DataFrame]]:
        pass

class DspyEvalSet(DspyDataset):
    def __init__(
        self, test_size, *args, **kwargs
    ) -> None:

        super().__init__(*args, **kwargs)
        self.test_size = test_size
        self.eval_mngr = kwargs.get("eval_mngr", None)
        self._create_train_test_split_and_ensure_labels()

    def _create_train_test_split_and_ensure_labels(self) -> None:
        """Perform a train/test split that ensure labels in `test` are also in `train`."""
        # Read the data
        data = self.eval_mngr.eval_set.data.toPandas()
        train_df, test_df = train_test_split(data, test_size=0.2, random_state=1)

        # Set DSPy class variables
        self._train = train_df.to_dict(orient="records")
        self._test = test_df.to_dict(orient="records")


class DspyIntrospectionSignature(dspy.Signature):
    previous_introspections: list[list[Feedback]] = dspy.InputField( desc="Previous introspections of the optimization loops feedback")
    previous_agg_quality_metrics: list[dict] = dspy.InputField(desc="Aggregated averages of the quality metrics return back from the evaluation harness from serverall optimization loops")
    current_quality_metrics: dict = dspy.InputField(desc="Aggregated averages of the quality metrics return back from the evaluation harness")
    current_introspection: list[Feedback] = dspy.InputField(desc="current introspection of the optimization loops feedback")
    ai_system_instruction: str = dspy.OutputField(desc="updated system instructions")
    rationale: str = dspy.OutputField(desc="rationale for the change to the system instructions")

class DspyIntrospection(dspy.Module):
    def __init__(self,genie_space_id: str, feedback: list[Feedback]=None):
        super().__init__()
        self.genie_space_id = genie_space_id
        self.genie_agent = GenieAgent(space_id=genie_space_id)
        self.introspect = dspy.Predict(DspyIntrospectionSignature)  # dspy.ChainOfThought(DspyIntrospectionSignature)
        self.llm = dspy.LM(config.runtime.INTROSPECT_AGENT_LLM_ENDPOINT, max_tokens=config.runtime.MAX_TOKENS)

    def forward(self, inputs:dict, reflection_data:AgentInput):
        with dspy.context(lm=self.llm):
            return dspy.Prediction(genie_query=self.genie_agent.invoke(inputs))


class DspyIntrospectionAgent(IntrospectiveAI):
    """ Implement Introspection AI with Dspy Agent Evaluation Judge Interface """

    def __init__(self, agent_config, judges_service:JudgeService, genie_service:GenieService, eval_mngr: EvalSetManager,**kwargs):
        super().__init__(**kwargs)
        self.agent_config = agent_config
        self.jmngr = judges_service
        self.genie_mngr = genie_service
        self.eval_mngr = eval_mngr
        self._configure()

    def _configure(self) -> None:
        lm = dspy.LM(config.runtime.INTROSPECT_AGENT_LLM_ENDPOINT, max_tokens=config.runtime.MAX_TOKENS)  # "databricks/databricks-claude-3-7-sonnet/"
        dspy.configure(lm=lm)
        self.llm = lm

    def compute_composite_metric(self, data_intelligence):
        pass

    def optimize(self,
                 data_intelligence,
                 mode: Literal["system_instructions", "column_descriptions", "data_model"] = "system_instructions"):

        match mode:
            case "system_instructions":
                self.system_instruction_introspection(agent_input)
            case "column_description":
                pass
            case "data_model":
                pass
            case _:
                raise ValueError("Invalid mode")

    def create_train_and_test_dataset(self) -> DspyDataset:
        data = self.eval_mngr.eval_set.data.toPandas()
        train_df, test_df = train_test_split(data, test_size=0.2, random_state=1)

        # Create train and test sets containing DSPy examples
        train_dataset = [example.with_inputs("description") for example in dataset.train]
        test_dataset = [example.with_inputs("description") for example in dataset.test]

    def gepa_introspection(self, config:dict):
        gepa = dspy.GEPA(
            metric=self.get_dspy_metric,
            auto="light",
            reflection_minibatch_size=config.get("reflection_minibatch_size", 5),
            reflection_lm=self.llm,
            num_threads=config.get("num_threads", 1),
            seed=1
        )

        with mlflow.start_run(run_name=f"DSPY GEPA Introspection Run", nested=False) as run:
            genie_space_id = self.genie_mngr.space_id
            GenieSqlGeneration = DspyIntrospection(genie_space_id)
            dataset = DspyEvalSet(test_size=0.2, eval_mngr=self.eval_mngr)

            train_dataset = [example.with_inputs("question") for example in dataset.train]
            #test_dataset = [example.with_inputs("question") for example in dataset.test]

            compiled_gepa = gepa.compile(
                GenieSqlGeneration,
                trainset=train_dataset,
            )

            compiled_gepa.save(f"compiled_gepa_{id}.json")


    def system_instruction_introspection(self, data_intelligence, strategy: Literal["MIPROv2", "GEPA"]="GEPA"):
        pass

    @staticmethod
    def calc_quality_score(metrics: dict, weight_map):
        num_metrics = len(metrics.keys())
        summed_metrics = [v * weight_map[k] for k, v in metrics.items() if weight_map.get(k, None)]
        quality_score = sum(summed_metrics) / num_metrics

        is_equal_to_1 = sum(weight_map.values()) == 1.0
        assert is_equal_to_1, "The sum of the weights must be equal to 1.0"
        return quality_score

    @staticmethod
    def calc_composite_metric(metrics: dict,
                              CHAOSLLAMA_METRICS_WEIGHT: float = .70,
                              CUSTOM_METRICS_WEIGHT: float = .30) -> float:
        # TODO: Make this more robust, to adjust for developer supplied custom scorers / guidelines.
        # Note: Chaos Llama out of the box metrics (sql_results_equivalence, sql_accuracy, correctness, sql_clauses_distribution_equivalence, relevance_to_query)
        # should always be weighted more, but leave room for customization

        _metrics = metrics.copy()
        _metrics.pop("agent/latency_seconds/mean")

        weighted_chaosllama_metrics_map = {
            'sql_results_equivalence/mean': 0.3125,
            'sql_accuracy/mean': 0.1875,
            'correctness/mean': 0.25,
            'sql_clauses_distribution_equivalence/mean': 0.125,
            'relevance_to_query/mean': .0625,
            'count_joins/mean': .0625
        }

        # TODO: Determine if HIGHER WEIGHTS for sql_result_set, sql_accuracy are more important
        # weighted_chaosllama_metrics_map = {
        # 'sql_results_equivalence/mean': 0.50,
        # 'sql_accuracy/mean': 0.25 ,
        # 'correctness/mean': .0625 ,
        # 'sql_clauses_distribution_equivalence/mean':.0625,
        # 'relevance_to_query/mean': .0625,
        # 'count_joins/mean': .0625
        # }

        # Create Weighted Custom Metrics Map
        metrics_list = _metrics.keys()
        custom_metrics = [
            m for m in metrics_list
            if m not in list(weighted_chaosllama_metrics_map.keys())
        ]

        num_custom_metrics = len(custom_metrics)
        weighted_custom_metrics_map = {m: (1 / num_custom_metrics) for m in custom_metrics}

        chaosllama_metrics_quality_score = CHAOSLLAMA_METRICS_WEIGHT * self.calc_quality_score(_metrics,
                                                                                               weighted_chaosllama_metrics_map)

        custom_metrics_quality_score = CUSTOM_METRICS_WEIGHT * self.calc_quality_score(_metrics,
                                                                                       weighted_custom_metrics_map)

        composite_metric_val = chaosllama_metrics_quality_score + custom_metrics_quality_score

        print(
            f"composite_metric_val={composite_metric_val:.2f}",
            f"chaosllama_metrics_quality_score={chaosllama_metrics_quality_score:.2f}",
            f"custom_metrics_quality_score={custom_metrics_quality_score:.2f}",
            sep="\n"
        )


        return composite_metric_val

    @staticmethod
    def collect_feedback(evalset: MosaicJudgesInput,
                         scorers: List[Callable],
                         guidelines: dict) -> List[DspyFeedback]:
        feedback = []
        for scorer in scorers:
            result = scorer(inputs=evalset.inputs, outputs=evalset.outputs, expectations=evalset.expectations)
            feedback.append(
                DspyFeedback(score=result.value, feedback=result.rationale)
            )

        for name, guideline in guidelines.items():
            result =  meets_guidelines(
                        name=name,
                        guidelines=guideline,
                        context=asdict(evalset)
            )

            feedback.append(
                DspyFeedback(score=result.value, feedback=result.rationale)
            )

        return feedback

    def get_dspy_metric(self,
                        example: dspy.Example,
                        pred: dspy.Prediction,
                        trace: Optional,
                        pred_name: str | None,
                        pred_trace: Optional):

        formatted_evalset = MosaicJudgesInput(
                            inputs=trace["inputs"],
                            outputs=pred.target,
                            expectations=dict(expected_response=example.target)
        )


        scorers = self.jmngr.scorers
        guidelines = config.scorers.global_guidelines["v2"]
        feedbacks = self.collect_feedback(formatted_evalset, scorers, guidelines)

        composite_metric = self.calc_composite_metric(feedbacks)
        format_feedback = "\n".join([fb.feedback for fb in feedbacks])
        return dspy.Prediction(score=composite_metric, feedback=format_feedback)

def test_dspy_introspection():
    jmngr = JudgeService().load_scorers_from_config()
    gmngr = GenieService(space_id=config.genie.RUNTIME_GENIE_SPACE_ID)  # üßû‚Äç‚ôÇÔ∏èGenie Service
    evmngr = EvalSetManager(table_name=f"{config.CATALOG}.{config.SCHEMA}.{config.EVAL_TABLE_NAME}",
                            limit=config.runtime.LIMIT,
                            consistency_factor=config.runtime.CONSISTENCY_FACTOR)
    agent_config = agent_config = AgentConfig(
            system_prompt=prompt_registry.INSTROSPECT_PROMPT_V3,
            endpoint=config.runtime.INTROSPECT_AGENT_LLM_ENDPOINT,
            llm_parameters={"temperature": 0.0, "max_tokens": config.runtime.MAX_TOKENS},
    )

    dspy_agent = DspyIntrospectionAgent(agent_config=agent_config,
                                        judges_service=jmngr,
                                        genie_service=gmngr,
                                        eval_mngr=evmngr)


    dspy_agent.gepa_introspection(config={"reflection_minibatch_size": 5, "num_threads": 1})


if __name__ == "__main__":
    test_dspy_introspection()