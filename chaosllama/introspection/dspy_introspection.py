"""
The purpose of this class is to inspect the data intelligence generated by mlflow.evaluate & lakehouse monitor to determine how to optimize the System prompt in One Shot

TODO: Attempt other strategies for metadata update. Here are a few strategies:
* Update prompt incrementally, one by one, per feedback of each quesiton in the evaluation dataset
"""

import dspy
from dspy import Adapter, Signature, BaseLM
from dspy.clients.lm import LM
from typing import Any
from databricks.connect import DatabricksSession
from dspy.datasets.dataset import Dataset as DspyDataset
from sklearn.model_selection import train_test_split
from typing import Dict, Optional, List, Callable, Literal, Union
import pandas as pd
from abc import ABC
from dataclasses import dataclass, asdict
from dotenv import dotenv_values

import mlflow
from mlflow.genai.scorers import Guidelines
from mlflow.genai.judges import meets_guidelines

from mlflow.entities import Feedback
import random

from chaosllama.services.evaluation_dataset import EvalSetManager
from chaosllama.services.genie import GenieAgent, GenieService
from chaosllama.profiles.config import config
from chaosllama.services.judges import JudgeService
from chaosllama.entities.models import (
                                        AgentConfig,
                                        IntrospectionManager,
                                        AgentInput,
                                        MosaicJudgesInput,
                                        DspyFeedback
                                        )
from chaosllama.prompts import registry as prompt_registry
from chaosllama.services.tracking import MLFlowExperimentManager


class IntrospectiveAI(ABC):
    def __init__(self, agent_config: AgentConfig):
        self.agent_config = agent_config

    def introspect(self, data_intelligence: pd.DataFrame):
        pass

    def optimize(self,
                 agent_input,
                 mode="system_instructions") -> Optional[Union[str, pd.DataFrame]]:
        pass

class DspyEvalSet(DspyDataset):
    def __init__(
        self,*args, eval_mngr: EvalSetManager = None, test_perc:float=0.30,**kwargs,
    ) -> None:

        super().__init__(*args, **kwargs)
        self.eval_mngr = eval_mngr #kwargs.get("eval_mngr", None)
        self.test_perc = test_perc
        self._create_train_test_split_and_ensure_labels()

    def _create_train_test_split_and_ensure_labels(self) -> None:
        """Perform a train/test split that ensure labels in `test` are also in `train`."""
        # Read the data
        data = self.eval_mngr.eval_set.data
        train_df, test_df = train_test_split(data, test_size=self.test_perc, random_state=1)

        # Set DSPy class variables
        self._train = train_df.to_dict(orient="records")
        self._test = test_df.to_dict(orient="records")

class DspyGenieAdapter(Adapter):
    def __init__(self, **kwargs):
        super().__init__(**kwargs)

    def __call__(
            self,
            lm: LM,
            lm_kwargs: dict[str, Any],
            signature: type[Signature],
            demos: list[dict[str, Any]],
            inputs: dict[str, Any],
    ) -> list[dict[str, Any]]:
        return lm(prompt=inputs["question"], **lm_kwargs)


class DspyGenieLM(BaseLM):
    def __init__(self, genie_space_id, *args, **kwargs):
        super().__init__(None,**kwargs)
        self.genie_space_id = genie_space_id
        self.genie_agent = GenieAgent(space_id=self.genie_space_id)

    def _format_messages(self, messages):
        return messages

    def _format_inputs(self, prompt:str, messages):
        """ Genie does not yet support programmatically updating the system instruction.
        Therefore, we will need to append the system instruction to the first question of the conversation.
        The prompt, will be the optimized prompt from the DSPY GEPA optimization routine.
        """
        new_input = f"{prompt}\n{self._format_messages(messages)}"
        return new_input

    def _format_outputs(self, outputs):
        return {}

    def __call__(self, prompt=None, messages=None, **kwargs):
        formatted_input = self._format_inputs(prompt, messages)
        return [dspy.Prediction(genie_sql_query=self.genie_agent.invoke(dict(question=formatted_input)))]

class DspyIntrospectionSignature(dspy.Signature):
    previous_introspections: list[list[Feedback]] = dspy.InputField( desc="Previous introspections of the optimization loops feedback")
    previous_agg_quality_metrics: list[dict] = dspy.InputField(desc="Aggregated averages of the quality metrics return back from the evaluation harness from serverall optimization loops")
    current_quality_metrics: dict = dspy.InputField(desc="Aggregated averages of the quality metrics return back from the evaluation harness")
    current_introspection: list[Feedback] = dspy.InputField(desc="current introspection of the optimization loops feedback")
    ai_system_instruction: str = dspy.OutputField(desc="updated system instructions")
    rationale: str = dspy.OutputField(desc="rationale for the change to the system instructions")

class DspyIntrospectionSignature_v2(dspy.Signature):
    """
    Optimize the system instructions to increase the accuracy of the SQL query that is Generated by Genie.


    """
    question: str = dspy.InputField( desc="Input question")
    genie_sql_query: str = dspy.OutputField(desc="Generated SQL Query")


class DspyIntrospection(dspy.Module):
    def __init__(self,genie_space_id: str, ):
        super().__init__()
        self.genie_space_id = genie_space_id
        self.genie_agent = GenieAgent(space_id=genie_space_id)
        self.introspect = dspy.Predict(DspyIntrospectionSignature_v2)  # dspy.ChainOfThought(DspyIntrospectionSignature)


    def forward(self, question: str):
        with dspy.context(lm=DspyGenieLM(self.genie_space_id, model_type="text"), adapter=DspyGenieAdapter()) as ctx:
            return self.introspect(question=question)


class DspyIntrospectionAgent(IntrospectiveAI):
    """ Implement Introspection AI with Dspy Agent Evaluation Judge Interface """

    def __init__(self, agent_config, judges_service:JudgeService, genie_service:GenieService, eval_mngr: EvalSetManager,**kwargs):
        super().__init__(agent_config=agent_config,**kwargs)
        self.agent_config = agent_config
        self.jmngr = judges_service
        self.genie_mngr = genie_service
        self.eval_mngr = eval_mngr
        self._configure()

    def _configure(self) -> None:
        lm = dspy.LM("databricks/"+config.runtime.INTROSPECT_AGENT_LLM_ENDPOINT, max_tokens=config.runtime.MAX_TOKENS)  # "databricks/databricks-claude-3-7-sonnet/"
        dspy.configure(lm=lm)
        self.llm = lm

    def compute_composite_metric(self, data_intelligence):
        pass

    def optimize(self,
                 data_intelligence,
                 mode: Literal["system_instructions", "column_descriptions", "data_model"] = "system_instructions"):

        match mode:
            case "system_instructions":
                self.system_instruction_introspection(agent_input)
            case "column_description":
                pass
            case "data_model":
                pass
            case _:
                raise ValueError("Invalid mode")

    def gepa_introspection(self, config:dict):
        gepa = dspy.GEPA(
            metric=self.get_dspy_metric,
            max_metric_calls=5, # controls the cost of the optimizaton run. How many times to call program
            #auto="light",
            reflection_minibatch_size=config.get("reflection_minibatch_size", 5),
            use_merge=True, # Whether to use the merge function to merge the optimized prompts from different pareto frontiers
            max_merge_invocations=10, # How many times to call the merge function
            reflection_lm=self.llm,
            num_threads=config.get("num_threads", 1),
            seed=1
        )

        with mlflow.start_run(run_name=f"DSPY GEPA Introspection Run", nested=False) as run:
            genie_space_id = self.genie_mngr.space_id
            GenieSqlGeneration = DspyIntrospection(genie_space_id)
            dataset = DspyEvalSet(test_perc=0.3, eval_mngr=self.eval_mngr)

            train_dataset = [ example.with_inputs("question") for example in dataset.train]
            test_dataset = [ example.with_inputs("question") for example in dataset.test]

            compiled_gepa = gepa.compile(
                GenieSqlGeneration,
                trainset=train_dataset,
                valset=test_dataset,
            )

            #compiled_gepa.save(f"compiled_gepa_{id}.json")

        optimized_prompt = compiled_gepa.history#[-1]["messages"][0]["content"]

        return optimized_prompt


    def system_instruction_introspection(self, data_intelligence, strategy: Literal["MIPROv2", "GEPA"]="GEPA"):
        pass

    @staticmethod
    def calc_quality_score(metrics: dict, weight_map):
        num_metrics = len(metrics.keys())
        summed_metrics = [v * weight_map[k] for k, v in metrics.items() if weight_map.get(k, None)]
        quality_score = sum(summed_metrics) / num_metrics

        is_equal_to_1 = sum(weight_map.values()) == 1.0
        assert is_equal_to_1, "The sum of the weights must be equal to 1.0"
        return quality_score

    @staticmethod
    def calc_composite_metric(metrics: dict,
                              CHAOSLLAMA_METRICS_WEIGHT: float = .70,
                              CUSTOM_METRICS_WEIGHT: float = .30) -> float:
        # TODO: Make this more robust, to adjust for developer supplied custom scorers / guidelines.
        # Note: Chaos Llama out of the box metrics (sql_results_equivalence, sql_accuracy, correctness, sql_clauses_distribution_equivalence, relevance_to_query)
        # should always be weighted more, but leave room for customization

        _metrics = metrics.copy()


        weighted_chaosllama_metrics_map = {}
        weighted_chaosllama_metrics_map = {
            'sql_results_equivalence/mean': 0.3125,
            'sql_accuracy/mean': 0.1875,
            'correctness/mean': 0.25,
            'sql_clauses_distribution_equivalence/mean': 0.125,
            'relevance_to_query/mean': .0625,
            'count_joins/mean': .0625
        }

        TODO: Determine if HIGHER WEIGHTS for sql_result_set, sql_accuracy are more important
        weighted_chaosllama_metrics_map = {
        'sql_results_equivalence/mean': 0.50,
        'sql_accuracy/mean': 0.25 ,
        'correctness/mean': .0625 ,
        'sql_clauses_distribution_equivalence/mean':.0625,
        'relevance_to_query/mean': .0625,
        'count_joins/mean': .0625
        }

        Create Weighted Custom Metrics Map
        metrics_list = _metrics.keys()
        custom_metrics = [
            m for m in metrics_list
            if m not in list(weighted_chaosllama_metrics_map.keys())
        ]

        num_custom_metrics = len(custom_metrics)
        weighted_custom_metrics_map = {m: (1 / num_custom_metrics) for m in custom_metrics}

        chaosllama_metrics_quality_score = CHAOSLLAMA_METRICS_WEIGHT * self.calc_quality_score(_metrics,
                                                                                               weighted_chaosllama_metrics_map)

        custom_metrics_quality_score = CUSTOM_METRICS_WEIGHT * self.calc_quality_score(_metrics,
                                                                                       weighted_custom_metrics_map)

        composite_metric_val = chaosllama_metrics_quality_score + custom_metrics_quality_score

        print(
            f"composite_metric_val={composite_metric_val:.2f}",
            f"chaosllama_metrics_quality_score={chaosllama_metrics_quality_score:.2f}",
            f"custom_metrics_quality_score={custom_metrics_quality_score:.2f}",
            sep="\n"
        )


        return composite_metric_val #.5 #random.choice([.5, .6 ,.7]) # composite_metric_val

    @staticmethod
    def collect_feedback(evalset: MosaicJudgesInput,
                         scorers: List[Callable],
                         guidelines: dict) -> List[DspyFeedback]:
        feedback = []
        for scorer in scorers:
            result = scorer(inputs=evalset.inputs, outputs=evalset.outputs, expectations=evalset.expectations)
            feedback.append(
                DspyFeedback(name=scorer.name, score=result.value, feedback=result.rationale)
            )

        for name, guideline in guidelines.items():
            result =  meets_guidelines(
                        name=name,
                        guidelines=guideline,
                        context=asdict(evalset)
            )

            feedback.append(
                DspyFeedback(name=name , score=result.value, feedback=result.rationale)
            )

        return feedback

    def get_dspy_metric(self,
                        example: dspy.Example,
                        pred: dspy.Prediction,
                        trace: Optional = None,
                        pred_name: str = None,
                        pred_trace: Optional = None):

        # Format the evalset for the judges
        print(f"Example: {example}")

        formatted_evalset = MosaicJudgesInput(
                            inputs=example.inputs,
                            outputs=pred.genie_sql_query,
                            expectations=dict(expected_response=example.ground_truth_sql)
        )


        scorers = self.jmngr.scorers
        guidelines = config.scorers.global_guidelines["v3"]
        feedbacks = self.collect_feedback(formatted_evalset, scorers, guidelines)

        composite_metric = self.calc_composite_metric(feedbacks)
        format_feedback = "\n".join([fb.feedback for fb in feedbacks])
        return dspy.Prediction(score=composite_metric, feedback=format_feedback)



def test_dspy_introspection():
    jmngr = JudgeService().load_scorers_from_config()
    gmngr = GenieService(space_id=config.genie.RUNTIME_GENIE_SPACE_ID)  # üßû‚Äç‚ôÇÔ∏èGenie Service
    evmngr = EvalSetManager(table_name=f"{config.CATALOG}.{config.SCHEMA}.{config.EVAL_TABLE_NAME}",
                            limit=5,
                            consistency_factor=config.runtime.CONSISTENCY_FACTOR)
    evmngr.prepare_evals(mode="local")


    agent_config = AgentConfig(
            system_prompt=prompt_registry.INSTROSPECT_PROMPT_V3,
            endpoint=config.runtime.INTROSPECT_AGENT_LLM_ENDPOINT,
            llm_parameters={"temperature": 0.0, "max_tokens": config.runtime.MAX_TOKENS},
    )

    EXPERIMENT_NAME = "TestGEPAChaosLlama"
    exp_mngr = MLFlowExperimentManager(experiment_path=config.mlflow.MLFLOW_EXPERIMENT_PATH).get_or_create_mlflow_experiment(EXPERIMENT_NAME)
    exp_mngr.set_experiment(f"{config.mlflow.MLFLOW_EXPERIMENT_PATH}{EXPERIMENT_NAME}")

    dspy_agent = DspyIntrospectionAgent(agent_config=agent_config,
                                        judges_service=jmngr,
                                        genie_service=gmngr,
                                        eval_mngr=evmngr)


    optimized_prompt = dspy_agent.gepa_introspection(config={"reflection_minibatch_size": 2, "num_threads": 1})
    return optimized_prompt


if __name__ == "__main__":
    test_dspy_introspection()