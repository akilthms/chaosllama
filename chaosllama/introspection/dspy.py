"""
The purpose of this class is to inspect the data intelligence generated by mlflow.evaluate & lakehouse monitor to determine how to optimize the System prompt in One Shot

TODO: Attempt other strategies for metadata update. Here are a few strategies:
* Update prompt incrementally, one by one, per feedback of each quesiton in the evaluation dataset
"""

import dspy
from mlflow.entities import Feedback
from typing import Dict
import pandas as pd
from abc import ABC
from databricks_langchain import ChatDatabricks
from chaosllama.entities.models import AgentConfig, AgentInput_v3, IntrospectionManager



class IntrospectionWorkflow():
    def __init__(self, agent):
        self.agent = agent

    def run(self, data_intelligence: list[Feedback]) -> Dict:
        return self.agent.optimize(data_intelligence, mode="system_instructions")


class InstrospectiveAI(ABC):
    def __init__(self, agent_config: AgentConfig):
        self.agent_config = agent_config

    def instrospect(self, data_intellegence: pd.DataFrame):
        pass

    def optimize(self,
                 agent_input,
                 mode="system_instructions") -> Optional[Union[str, pd.DataFrame]]:
        pass


class IntrospectionAIAgent():
    """The purpose of this class is to inspect the data intelligence generated by mlflow.evaluate & lakehouse monitor to determine how to optimize the System prompt in One Shot
    """

    def __init__(self, agent_config: AgentConfig):
        self.agent_config = agent_config
        self.agent = None
        # self.ucmg = ucmg

    def create_agent(self):
        chat_model = ChatDatabricks(endpoint=self.agent_config.endpoint, **self.agent_config.llm_parameters)

        self.agent = (
                self.agent_config.system_prompt |
                chat_model |
                cll_prompts.introspection_parser
        )

        return self

    def retry_introspection(max_retries: int = 2, delay: int = 1):
        def decorator(func):
            def wrapper(*args, **kwargs):
                attempt = 0
                while attempt < max_retries:
                    try:
                        res = func(*args, **kwargs)
                        return res
                    except OutputParserException as e:
                        print(
                            f"\tðŸ§ âŒ Introspection Attempt {attempt} failed with Exception: {e} attempting retry {max_retries - attempt} more times")
                        attempt += 1
                        if attempt < max_retries:
                            print("\t", f"Retrying in {delay} seconds...")
                            time.sleep(delay + attempt)  # Increasing back off
                        else:
                            print(f"ðŸ˜µ All {max_retries} attempts failed.")
                            return None
                    finally:
                        pass

            return wrapper

        return decorator

    @mlflow.trace(name="ðŸ”Ž Introspect", span_type=SpanType.TOOL)
    @retry_introspection()
    def introspect(self, inputs: AgentInput_v3, instrmg: IntrospectionManager = None):
        if not self.agent:
            self.create_agent()

        updated_metadata: dict = self.agent.invoke(asdict(inputs))

        # TODO: Store metadata about introspection to DELTA tables
        return updated_metadata

    @mlflow.trace(name="ðŸ”§ Optimize", span_type=SpanType.TOOL)
    def optimize(self, agent_input, mode="system_instructions"):
        match mode:
            case "system_instructions":
                self.introspect(agent_input)
            case "column_description":
                pass
            case "data_model":
                pass
            case _:
                raise ValueError("Invalid mode")

    def log_agent(self):
        """ The purpose of this method is to log the agent to mlflow"""
        pass




# class MosaicEvaluationAgent():
#     """ Implement Introspection AI with Mosaic Agent Evaluation Judge Interface """
#     pass
#
#
# agent_config = AgentConfig(
#     system_prompt=cll_prompts.INSTROSPECT_PROMPT_V3,
#     endpoint=INTROSPECT_AGENT_LLM_ENDPOINT,
#     llm_parameters={
#         "temperature": 0.0,
#         "max_tokens": MAX_TOKENS,
#         "optimization_strategy": "MIPROv2"
#     }
# )

class DspyInstrospectionSignature(dspy.Signature):
    """Instrospect on a set of feedbacks returned from mosaic evaluation harness and suggest"""
    previous_introspections: list[list[Feedback]] = dspy.InputField(
        desc="Previous introspections of the optimization loops feedback")
    previous_agg_quality_metrics: list[dict] = dspy.InputField(
        desc="Aggregated averages of the quality metrics return back from the evaluation harness from serverall optimization loops")
    current_quality_metrics: dict = dspy.InputField(
        desc="Aggregated averages of the quality metrics return back from the evaluation harness")
    current_introspection: list[Feedback] = dspy.InputField(
        desc="current introspection of the optimization loops feedback")
    ai_system_instruction: str = dspy.OutputField(desc="updated system instructions")
    rationale: str = dspy.OutputField(desc="rationale for the change to the system instructions")


class DspyInstropection(dspy.Module):
    def __init__(self):
        self.introspect = dspy.ChainOfThought(DspyInstrospectionSignature)

    def forward(self, data_intelligence: dict):
        return self.introspect(**data_intelligence)


class DspyIntrospectionAgent(InstrospectiveAI):
    """ Implement Introspection AI with Dspy Agent Evaluation Judge Interface """

    def __init__(self, agent_config, **kwargs):
        super().__init__(**kwargs)
        self.agent_config = agent_config
        self._configure()

    def _configure(self) -> None:
        lm = dspy.LM(self.agent_config.endpoint)  # "databricks/databricks-claude-3-7-sonnet/"
        dspy.configure(lm=lm)

    def compute_composite_metric(self, data_intelligence):
        pass

    def optimize(self,
                 data_intelligence,
                 mode: Literal["system_instructions", "column_descriptions", "data_model"] = "system_instructions"):

        match mode:
            case "system_instructions":
                self.system_instruction_introspection(agent_input)
            case "column_description":
                pass
            case "data_model":
                pass
            case _:
                raise ValueError("Invalid mode")

    def system_instruction_introspection(self, data_intelligence):
        num_questions = data_intelligence["num_questions"]  # Grab number of questions from feedback

        # Configure the Dspy optimization strategy

        OptStrat = dspy.MIPROv2 if self.agent_config.optimization_strategy == "MIPROv2" else dspy.CORPO
        mlflow.log_param("optimization_strategy", OptStrat.__name__)

        teleprompter = OptStrat(metric=self.compute_composite_metric,
                                trainset=trainset,
                                val
        auto = "medium",
        num_threads = num_questions)

        trainset =
        valset =

        optimized_rag = teleprompter.compile(DspyInstropection(),
                                             trainset=trainset,
                                             valset=valset
        max_bootstrapped_demos = 2, max_labeled_demos = 2)

        @staticmethod
        def calc_quality_score(metrics: dict, weight_map):
            num_metrics = len(metrics.keys())
            summed_metrics = [v * weight_map[k] for k, v in metrics.items() if weight_map.get(k, None)]
            quality_score = sum(summed_metrics) / num_metrics

            is_equal_to_1 = sum(weight_map.values()) == 1.0
            assert is_equal_to_1, "The sum of the weights must be equal to 1.0"
            return quality_score

        @staticmethod
        def calc_composite_metric(metrics: dict,
                                  CHAOSLLAMA_METRICS_WEIGHT: float = .70,
                                  CUSTOM_METRICS_WEIGHT: float = .30) -> float:
            # TODO: Make this more robust, to adjust for developer supplied custom scorers / guidelines.
            # Note: Chaos Llama out of the box metrics (sql_results_equivalence, sql_accuracy, correctness, sql_clauses_distribution_equivalence, relevance_to_query)
            # should always be weighted more, but leave room for customization

            _metrics = metrics.copy()
            _metrics.pop("agent/latency_seconds/mean")

            weighted_chaosllama_metrics_map = {
                'sql_results_equivalence/mean': 0.3125,
                'sql_accuracy/mean': 0.1875,
                'correctness/mean': 0.25,
                'sql_clauses_distribution_equivalence/mean': 0.125,
                'relevance_to_query/mean': .0625,
                'count_joins/mean': .0625
            }

            # TODO: Determine if HIGHER WEIGHTS for sql_result_set, sql_accuracy are more important
            # weighted_chaosllama_metrics_map = {
            # 'sql_results_equivalence/mean': 0.50,
            # 'sql_accuracy/mean': 0.25 ,
            # 'correctness/mean': .0625 ,
            # 'sql_clauses_distribution_equivalence/mean':.0625,
            # 'relevance_to_query/mean': .0625,
            # 'count_joins/mean': .0625
            # }

            # Create Weighted Custom Metrics Map
            metrics_list = _metrics.keys()
            custom_metrics = [
                m for m in metrics_list
                if m not in list(weighted_chaosllama_metrics_map.keys())
            ]

            num_custom_metrics = len(custom_metrics)
            weighted_custom_metrics_map = {m: (1 / num_custom_metrics) for m in custom_metrics}

            chaosllama_metrics_quality_score = CHAOSLLAMA_METRICS_WEIGHT * self.calc_quality_score(_metrics,
                                                                                                   weighted_chaosllama_metrics_map)

            custom_metrics_quality_score = CUSTOM_METRICS_WEIGHT * self.calc_quality_score(_metrics,
                                                                                           weighted_custom_metrics_map)

            composite_metric_val = chaosllama_metrics_quality_score + custom_metrics_quality_score

            print(
                f"composite_metric_val={composite_metric_val:.2f}",
                f"chaosllama_metrics_quality_score={chaosllama_metrics_quality_score:.2f}",
                f"custom_metrics_quality_score={custom_metrics_quality_score:.2f}",
                sep="\n"
            )

            return composite_metric_val

def test_dspy_introspection():
    dspy_agent = DspyIntrospectionAgent(agent_config=agent_config)
    results = dspy.optimize(data_intelligence, mode="system_instructions")
    print(results)

