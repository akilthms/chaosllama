"""
The purpose of this class is to inspect the data intelligence generated by mlflow.evaluate & lakehouse monitor to determine how to optimize the System prompt in One Shot

TODO: Attempt other strategies for metadata update. Here are a few strategies:
* Update prompt incrementally, one by one, per feedback of each quesiton in the evaluation dataset
"""


class IntrospectionWorkflow():
    def __init__(self, agent):
        self.agent = agent

    def run(self, data_intelligence: list[Feedback]) -> Dict:
        return self.agent.optimize(data_intelligence, mode="system_instructions")


class InstrospectiveAI(ABC):
    def __init__(self, agent_config: AgentConfig):
        self.agent_config = agent_config

    def instrospect(self, data_intellegence: pd.DataFrame):
        pass

    def optimize(self,
                 agent_input,
                 mode="system_instructions") -> Optional[Union[str, pd.DataFrame]]:
        pass


class IntrospectionAIAgent():
    """The purpose of this class is to inspect the data intelligence generated by mlflow.evaluate & lakehouse monitor to determine how to optimize the System prompt in One Shot
    """

    def __init__(self, agent_config: AgentConfig):
        self.agent_config = agent_config
        self.agent = None
        # self.ucmg = ucmg

    def create_agent(self):
        chat_model = ChatDatabricks(endpoint=self.agent_config.endpoint, **self.agent_config.llm_parameters)

        self.agent = (
                self.agent_config.system_prompt |
                chat_model |
                cll_prompts.introspection_parser
        )

        return self

    def retry_introspection(max_retries: int = 2, delay: int = 1):
        def decorator(func):
            def wrapper(*args, **kwargs):
                attempt = 0
                while attempt < max_retries:
                    try:
                        res = func(*args, **kwargs)
                        return res
                    except OutputParserException as e:
                        print(
                            f"\t🧠❌ Introspection Attempt {attempt} failed with Exception: {e} attempting retry {max_retries - attempt} more times")
                        attempt += 1
                        if attempt < max_retries:
                            print("\t", f"Retrying in {delay} seconds...")
                            time.sleep(delay + attempt)  # Increasing back off
                        else:
                            print(f"😵 All {max_retries} attempts failed.")
                            return None
                    finally:
                        pass

            return wrapper

        return decorator

    @mlflow.trace(name="🔎 Introspect", span_type=SpanType.TOOL)
    @retry_introspection()
    def introspect(self, inputs: AgentInput_v3, instrmg: IntrospectionManager = None):
        if not self.agent:
            self.create_agent()

        updated_metadata: dict = self.agent.invoke(asdict(inputs))

        # TODO: Store metadata about introspection to DELTA tables
        return updated_metadata

    @mlflow.trace(name="🔧 Optimize", span_type=SpanType.TOOL)
    def optimize(self, agent_input, mode="system_instructions"):
        match mode:
            case "system_instructions":
                self.introspect(agent_input)
            case "column_description":
                pass
            case "data_model":
                pass
            case _:
                raise ValueError("Invalid mode")

    def log_agent(self):
        """ The purpose of this method is to log the agent to mlflow"""
        pass


class DspyInstrospectionSignature(dspy.Signature):
    """Instrospect on a set of feedbacks returned from mosaic evaluation harness and suggest"""
    previous_introspections: list[list[Feedback]] = dspy.InputField(
        desc="Previous introspections of the optimization loops feedback")
    previous_agg_quality_metrics: list[dict] = dspy.InputField(
        desc="Aggregated averages of the quality metrics return back from the evaluation harness from serverall optimization loops")
    current_quality_metrics: dict = dspy.InputField(
        desc="Aggregated averages of the quality metrics return back from the evaluation harness")
    current_introspection: list[Feedback] = dspy.InputField(
        desc="current introspection of the optimization loops feedback")
    ai_system_instruction: str = dspy.OutputField(desc="updated system instructions")
    rationale: str = dspy.OutputField(desc="rationale for the change to the system instructions")


class DspyInstropection(dspy.Module):
    def __init__(self):
        self.introspect = dspy.ChainOfThought(DspyInstrospectionSignature)

    def forward(self, data_intelligence: dict):
        return self.introspect(**data_intelligence)


class DspyIntrospectionAgent(InstrospectiveAI):
    """ Implement Introspection AI with Dspy Agent Evaluation Judge Interface """

    def __init__(self, agent_config, **kwargs):
        super().__init__(**kwargs)
        self.agent_config = agent_config
        self._configure()
        self.optimization_strategies = [dspy.MIPROv2, dspy.CORPO]

    def _configure(self) -> None:
        lm = dspy.LM(self.agent_config.endpoint)  # "databricks/databricks-claude-3-7-sonnet/"
        dspy.configure(lm=lm)

    def compute_composite_metric(self, data_intelligence):
        pass

    def optimize(self, data_intelligence, test_size=.50):
        num_questions = data_intelligence["num_questions"]  # Grab number of questions from feedback

        # Configure the Dspy optimization strategy

        OptStrat = dspy.MIPROv2 if self.agent_config.optimization_strategy == "MIPROv2" else dspy.CORPO
        mlflow.log_param("optimization_strategy", OptStrat.__name__)

        teleprompter = OptStrat(metric=dspy.evaluate.SemanticF1(decompositional=True),
                                auto="medium",
                                num_threads=num_questions)

        dspy_data = [dspy.Example(**asdict(di)).with_inputs('question') for di in data_intelligence]
        trainset, valset = train_test_split(dspy_data, test_size=test_size, random_state=42)

        optimized_genie_space = teleprompter.compile(DspyInstropection(),
                                                     trainset=trainset,
                                                     valset=valset,
                                                     max_bootstrapped_demos=2,
                                                     max_labeled_demos=2)
        return optimized_genie_space

    # # Import the optimizer
    # # Initialize optimizer
    # teleprompter = MIPROv2(
    #     metric=gsm8k_metric,
    #     auto="light", # Can choose between light, medium, and heavy optimization runs
    # )

    # # Optimize program
    # print(f"Optimizing program with MIPRO...")
    # optimized_program = teleprompter.compile(
    #     program.deepcopy(),
    #     trainset=trainset,
    #     max_bootstrapped_demos=3,
    #     max_labeled_demos=4,
    #     requires_permission_to_run=False,
    # )

    # # Save optimize program for future use
    # optimized_program.save(f"mipro_optimized")

    # # Evaluate optimized program
    # print(f"Evaluate optimized program...")
    # evaluate(optimized_program, devset=devset[:])


class MosaicEvaluationAgent():
    """ Implement Introspection AI with Mosaic Agent Evaluation Judge Interface """
    pass


agent_config = AgentConfig(
    system_prompt=cll_prompts.INSTROSPECT_PROMPT_V3,
    endpoint=INTROSPECT_AGENT_LLM_ENDPOINT,
    llm_parameters={
        "temperature": 0.0,
        "max_tokens": MAX_TOKENS,
        "optimization_strategy": "MIPROv2"
    }
)


def test_dspy_introspection():
    dspy_agent = DspyIntrospectionAgent(agent_config=agent_config)
    results = dspy.optimize(data_intelligence, mode="system_instructions")
    print(results)

