{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from chaosllama.chaos import ChaosLlama, ChaosLlamaServicesConfig\n",
    "from chaosllama.entities.models import AgentConfig\n",
    "from chaosllama.services import genie, mosaic, unity_catalog, judges\n",
    "from chaosllama.services.evaluation_dataset import EvalSetManager\n",
    "from chaosllama.profiles.config import config\n",
    "from chaosllama.services.tracking import MLFlowExperimentManager\n",
    "from chaosllama.scorers.scorers import eval_sql_clauses_distro, eval_query_results\n",
    "import chaosllama.prompts.registry as prompt_registry\n",
    "import pyfiglet\n",
    "import rich\n",
    "from rich.console import Console"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "    # ü™µLogging Configuration\n",
    "\n",
    "    # TODO: Add logging configuration\n",
    "\n",
    "    banner = pyfiglet.figlet_format(\"Chaos Llama ü¶ô\")\n",
    "    print(banner)\n",
    "\n",
    "    # üßë‚Äçüç≥ Prepare Evaluation Data Set\n",
    "    evmngr = EvalSetManager(table_name=f\"{config.CATALOG}.{config.SCHEMA}.{config.EVAL_TABLE_NAME}\",\n",
    "                        limit=config.runtime.LIMIT,\n",
    "                        consistency_factor=config.runtime.CONSISTENCY_FACTOR)\n",
    "    evmngr.prepare_evals(mode=\"synthetic\")\n",
    "    evmngr.write_evalset()\n",
    "\n",
    "    # üßë‚Äçüî¨Set MLFLow Experiment\n",
    "    exp_mngr = MLFlowExperimentManager(experiment_path=config.mlflow.MLFLOW_EXPERIMENT_PATH).get_or_create_mlflow_experiment(config.mlflow.MLFLOW_RUNTIME_EXPERIMENT)\n",
    "\n",
    "    # üßë‚ÄçüîßÔ∏èConfigure Services\n",
    "    jmngr = judges.JudgeService(scorers=[eval_sql_clauses_distro,eval_query_results]) #üßë‚Äç‚öñÔ∏èJudges eval_query_results\n",
    "    mlfmngr = mosaic.MosaicEvalService(eval_manager=evmngr,judge_manager=jmngr, experiment_id=exp_mngr.experiment_id) # üß™Mosaic Evaluations\n",
    "    gmngr = genie.GenieService(space_id=config.genie.RUNTIME_GENIE_SPACE_ID) # üßû‚Äç‚ôÇÔ∏èGenie Service\n",
    "    ucmngr = unity_catalog.UCService(catalog=config.CATALOG, schema=config.SCHEMA) # ü§ù Unity Catalog Manager\n",
    "    agent_config = AgentConfig(\n",
    "            system_prompt=prompt_registry.INSTROSPECT_PROMPT_V3,\n",
    "            endpoint=config.runtime.INTROSPECT_AGENT_LLM_ENDPOINT,\n",
    "            llm_parameters={\"temperature\": 0.0, \"max_tokens\": config.runtime.MAX_TOKENS},\n",
    "    )\n",
    "\n",
    "    # ‚öôÔ∏èü¶ôConfigure Chaos Llama\n",
    "    chaos_config = ChaosLlamaServicesConfig(\n",
    "        mlflow_manager=mlfmngr,\n",
    "        genie_manager=gmngr,\n",
    "        uc_manager=ucmngr,\n",
    "        agent_config=agent_config\n",
    "    )\n",
    "\n",
    "    chaos_llama = ChaosLlama(config=chaos_config)\n",
    "\n",
    "    data_intelligence, mlflow_parent_run = chaos_llama.run(\n",
    "        epochs=config.runtime.EPOCHS,\n",
    "        is_test=config.runtime.DEBUG,\n",
    "        is_cached=config.runtime.IS_CACHED,\n",
    "        run_baseline=config.runtime.RUN_BASELINE,\n",
    "        run_null_hypothesis=config.runtime.RUN_NULL_HYPOTHESIS,\n",
    "    )"
   ],
   "id": "162819139f31298"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
