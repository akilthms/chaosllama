# ü§ù Unity Catalog
CATALOG: &CATALOG 
SCHEMA: &SCHEMA 
CHAOS_ANALYTICS_TABLE_TEMPLATE: "${CATALOG}.${SCHEMA}.chaos_analytics"
EVAL_TABLE_NAME: "eval_set"  # You can set this dynamically if needed
CHAOS_LLAMA_PROFILE: "dev"


# üèÉ Runtime Configuration
runtime:
  MAX_TOKENS: 800
  LIMIT: 2 # Maximum number of questions for evaluation dataset
  CONSISTENCY_FACTOR: 1
  EPOCHS: 2
  BATCH_SIZE: 5
  N_JOBS: 2
  VALIDATION_SET_LIMIT: 20
  REFRESH_DASHBOARD: False
  RUN_BASELINE: False
  RUN_NULL_HYPOTHESIS: False
  INTROSPECTION_LOOKBACK: 3
  DEBUG: false
  IS_TRIGGERED_FROM_CHECKPOINT: false # Start the Chaos Llama run from a checkpoint
  INTROSPECT_AGENT_LLM_ENDPOINT: &INTROSPECT_AGENT_LLM_ENDPOINT "databricks-claude-3-7-sonnet"
  BASELINE_CATALOG: ""
  BASELINE_SCHEMA: ""
  IS_CACHED: False # Use cached results for the evaluation dataset

# üßû‚Äç‚ôÇÔ∏è Genie Configuration
genie:
  RUNTIME_GENIE_SPACE_ID: "01f02a9a8ad0145d85f7fd4da2a27322"
  NULL_HYPOTHESIS_GENIE_SPACE_ID: "01f0086525d917ddb7527333c5af68ba"
  TEST_GENIE_SPACE_ID: ""
  BASELINE_GENIE_SPACE_ID: ""
  VALIDATION_GENIE_SPACE_ID: ""

# ü§ñ AI Configuration
introspection:
  INTROSPECT_AGENT_LLM_ENDPOINT: *INTROSPECT_AGENT_LLM_ENDPOINT
  INTROSPECT_AGENT_LLM_CHOICES:
    - databricks-claude-3-7-sonnet
    - databricks-claude-sonnet-4
    - databricks-llama-4-maverick
    - databricks-claude-opus-4


# ü§ñ Small LLM Configuration
SMALL_LLM_ENDPOINTS: "databricks-meta-llama-3-1-405b-instruct"
SMALL_LLM_ENDPOINTS_CHOICES:
  - databricks-meta-llama-3-1-405b-instruct
  - databricks-mixtral-8x7b-instruct
  - databricks-meta-llama-3-1-8b-instruct
  - databricks-llama-4-maverick

# üßë‚Äç‚öñÔ∏è Judges Configuration
# Add your custom scrorer here by function name
scorers:
  custom_scorers:
    - eval_query_results
    - eval_sql_clauses_distro

  QUALITY_THRESHOLD: 0.90
  METRICS_DEFINITION: |
    SQL Accuracy: The response query does not match the expected query in terms of metrics, countries, and years.
    Dimension Ambiguity: The response uses different dimension tables than those specified in the request.
    Correctness: The response does not match the context of the question or the expected response.
    Guideline Adherence: The response does not address issues with multiple columns and omits necessary filters.

  global_guidelines:
    v1:
      sql_accuracy: ["Ensure that response and the ground truth are semantically equivalent in ansi spark sql"]
      count_joins: ["Count the number of joins are the same in both queries."]
      dimension_ambiguity: ["Identify if the query chooses the wrong dimension table"]
      has_select: ["Ensure that the expected response has a sql SELECT statement , ignoring case, in the ansi sql query"]
      financial_planning: ["For every generated query the following dimensions MUST BE PRESENT. Dimensions: [scenario, report_type, year]"]

    v2:
      sql_semantic_equivalence: [ "Ensure candidate and ground truth are semantically equivalent (allow commutativity, aliasing, harmless syntactic variants)." ]
      projection_correctness: [ "Selected columns/expressions match the question‚Äôs intent and meaning." ]
      aggregation_grain: [ "GROUP BY keys produce the intended granularity (neither over- nor under-aggregated)." ]
      filter_logic_accuracy: [ "Filters match intended attributes, operators, values, and boundary conditions." ]
      time_logic_and_grain: [ "Correct time windows, inclusivity, buckets, and (if relevant) timezone assumptions." ]
      numerical_precision_and_nulls: [ "Correct casts, rounding, divide-by-zero protection, and NULL handling." ]
      join_count_consistency: [ "Join count is minimal/appropriate; extra joins do not alter results." ]
      window_function_correctness: [ "Window PARTITION BY/ORDER BY match the intended metric semantics." ]
      subquery_cte_clarity: [ "Subqueries/CTEs structure logic correctly without altering semantics." ]
      dimension_table_selection: [ "Correct choice of dimension(s) for the business meaning." ]
      measure_definition_conformity: [ "Measures conform to business definitions (e.g., revenue net of discounts)." ]
      unnecessary_complexity: [ "Prefer simpler equivalent queries; avoid complexity that risks drift." ]
      induced_follow_up_question: [ "Would the query‚Äôs logic/assumptions prompt a follow-up question for clarity?" ]
    v3:
        sql_semantic_equivalence: [ "Ensure candidate and ground truth are semantically equivalent (allow commutativity, aliasing, harmless syntactic variants)." ]



mlflow:
  MLFLOW_RUNTIME_EXPERIMENT: "ü¶ôChaosLlama v3"
  MLFLOW_EXPERIMENT_PATH: 
  BEST_MLFLOW_RUN: ""
  MLFLOW_EXPERIMENTS:
    - "ü¶ôChaosLlama v2"




