# ü§ù Unity Catalog
CATALOG: &CATALOG "retail_consumer_goods"
SCHEMA: &SCHEMA "store_ops"
CHAOS_ANALYTICS_TABLE_TEMPLATE: "${CATALOG}.${SCHEMA}.chaos_analytics"
EVAL_TABLE_NAME: "eval_set"  # You can set this dynamically if needed
CHAOS_LLAMA_PROFILE: "dev"


# üèÉ Runtime Configuration
runtime:
  MAX_TOKENS: 200
  LIMIT: 2 # Maximum number of questions for evaluation dataset
  CONSISTENCY_FACTOR: 1
  EPOCHS: 2
  BATCH_SIZE: 5
  N_JOBS: 2
  VALIDATION_SET_LIMIT: 20
  REFRESH_DASHBOARD: False
  RUN_BASELINE: False
  RUN_NULL_HYPOTHESIS: False
  INTROSPECTION_LOOKBACK: 3
  DEBUG: false
  IS_TRIGGERED_FROM_CHECKPOINT: false # Start the Chaos Llama run from a checkpoint
  INTROSPECT_AGENT_LLM_ENDPOINT: &INTROSPECT_AGENT_LLM_ENDPOINT "databricks-claude-3-7-sonnet"
  BASELINE_CATALOG: ""
  BASELINE_SCHEMA: ""
  IS_CACHED: False # Use cached results for the evaluation dataset

# üßû‚Äç‚ôÇÔ∏è Genie Configuration
genie:
  RUNTIME_GENIE_SPACE_ID: "01f05dd06c421ad6b522bf7a517cf6d2"
  NULL_HYPOTHESIS_GENIE_SPACE_ID: ""
  TEST_GENIE_SPACE_ID: ""
  BASELINE_GENIE_SPACE_ID: ""
  VALIDATION_GENIE_SPACE_ID: ""

# ü§ñ AI Configuration
introspection:
  INTROSPECT_AGENT_LLM_ENDPOINT: *INTROSPECT_AGENT_LLM_ENDPOINT
  INTROSPECT_AGENT_LLM_CHOICES:
    - databricks-claude-3-7-sonnet
    - databricks-claude-sonnet-4
    - databricks-llama-4-maverick
    - databricks-claude-opus-4


# ü§ñ Small LLM Configuration
SMALL_LLM_ENDPOINTS: "databricks-meta-llama-3-1-405b-instruct"
SMALL_LLM_ENDPOINTS_CHOICES:
  - databricks-meta-llama-3-1-405b-instruct
  - databricks-mixtral-8x7b-instruct
  - databricks-meta-llama-3-1-8b-instruct
  - databricks-llama-4-maverick

# üßë‚Äç‚öñÔ∏è Judges Configuration
scorers:
  QUALITY_THRESHOLD: 0.90
  METRICS_DEFINITION: |
    SQL Accuracy: The response query does not match the expected query in terms of metrics, countries, and years.
    Dimension Ambiguity: The response uses different dimension tables than those specified in the request.
    Correctness: The response does not match the context of the question or the expected response.
    Guideline Adherence: The response does not address issues with multiple columns and omits necessary filters.

  global_guidelines:
    v1:
      sql_accuracy: ["Ensure that response and the ground truth are semantically equivalent in ansi spark sql"]
      count_joins: ["Count the number of joins are the same in both queries."]
      dimension_ambiguity: ["Identify if the query chooses the wrong dimension table"]
      has_select: ["Ensure that the expected response has a sql SELECT statement , ignoring case, in the ansi sql query"]
      financial_planning: ["For every generated query the following dimensions MUST BE PRESENT. Dimensions: [scenario, report_type, year]"]

    v2:
      sql_accuracy: ["Ensure that response and the ground truth are semantically equivalent in ansi spark sql"]
      count_joins: ["Count the number of joins are the same in both queries."]
      has_select: ["Ensure that the expected response has a sql SELECT statement , ignoring case, in the ansi sql query"]

mlflow:
  MLFLOW_RUNTIME_EXPERIMENT: "ü¶ôChaos Llama"
  MLFLOW_EXPERIMENT_PATH: /Workspace/Users/akil.thomas@databricks.com/
  BEST_MLFLOW_RUN: ""
  MLFLOW_EXPERIMENTS:
    - "ü¶ôChaos Llama"




